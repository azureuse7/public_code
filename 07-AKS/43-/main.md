## 1\. Baseline AKS – what is already encrypted?

### 1.1 Control-plane 

*   **API server endpoints**
    
    *   Kubernetes API is exposed over HTTPS (TLS) for kubectl, controllers, and node kubelets.
        
    *   Certificates and CAs are managed by AKS; you do not manage those X.509 certs directly.
        
*   **Node ↔ API server**
    
    *   Kubelet on each node talks to the API server over TLS, either over public or private IP depending on your cluster configuration (private cluster, API server VNet integration, etc.). [Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/api-server-vnet-integration?utm_source=chatgpt.com)
        

So all control-plane communication is already TLS-encrypted, with a Microsoft-managed CA.

### 1.2 Data at rest

*   **OS & data disks**
    
    *   Azure disks are encrypted at rest with Storage Service Encryption.
        
*   **Node host cache / temp disks**
    
    *   With “encryption at host”, even the host cache and temp disks are encrypted 
        
*   **etcd (cluster state, secrets)**
    
    *   By default, AKS uses encryption at rest.
        


All of this is independent of ACNS.

### 1.3 Default data-plane (pod ↔ pod, node ↔ node)

Without ACNS / WireGuard:

*   **Pod ↔ pod across nodes**
    
    *   Traffic is routed by your chosen CNI:
        
        *   Azure CNI (classic)
            
        *   Azure CNI Overlay
            
        *   Azure CNI powered by Cilium (but with ACNS disabled)
            
        *   Kubenet / BYO CNI
            
    *   By default this traffic is **not transparently encrypted at the network layer**. It’s just routed L3 over the VNet.
        
    *   If you want encryption, you normally rely on:
        
        *   Application-level TLS (HTTPS, TLS, etc.), or
            
        *   A service mesh with mTLS (Istio, Linkerd, etc.).
            
*   **Pod ↔ pod on the same node**
    
    *   Stays on the node; bridged or routed locally by the CNI.
        
    *   Again, not automatically encrypted unless the application/service mesh does it.
        
*   **Node ↔ node**
    
    *   Standard IP routing over the VNet, unencrypted at L3 unless something else kicks in.
        

### 1.4 Optional: Azure Virtual Network Encryption (VNet encryption)

Separately from ACNS, Azure offers **VNet encryption** for VM traffic:

*   This creates DTLS tunnels between NICs so that **all private IP traffic between VMs in the VNet** is encrypted (and across peered VNets). [Microsoft Learn](https://learn.microsoft.com/en-us/azure/virtual-network/virtual-network-encryption-overview?utm_source=chatgpt.com)
    
*   For AKS:
    
    *   Supported for clusters using Azure CNI (regular or overlay) and Kubenet; node and pod traffic that traverses the VNet is encrypted at the VM level. [Microsoft Learn](https://learn.microsoft.com/en-us/azure/virtual-network/virtual-network-encryption-overview?utm_source=chatgpt.com)
        
    *   Not compatible with some features (e.g. API Server VNet integration on certain SKUs). [Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/api-server-vnet-integration?utm_source=chatgpt.com)
        

So, **baseline AKS gives you encryption for control-plane and storage**, and optionally VNet-level encryption. But **it does not natively encrypt pod-to-pod traffic at the Kubernetes/networking layer**.

* * *


## 2\. What CA does it use?

There are two very different “crypto stories” here:

### a) WireGuard in-transit encryption (ACNS feature)

## What Gets Encrypted

**Encrypted:**

*   Inter-node pod traffic (pod on node A → pod on node B)
    

**Not Encrypted:**

*   Same-node pod traffic (pods on the same node talking to each other)
    
*   Node-to-node traffic generated by the node itself
    

WireGuard on AKS **does not use a traditional x.509 CA at all**. Instead: [Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/container-network-security-wireguard-encryption-concepts)

*   Each node generates its own **WireGuard key pair** in memory during initialization.
    
*   The **public key** is published via the CiliumNode CR’s annotations [network.cilium.io/wg-pub-key](http://network.cilium.io/wg-pub-key)
    
*   The **private key** never leaves the node; keys are rotated automatically (the doc states ~120 seconds).
    
*   All of this is orchestrated by Azure/Cilium – you don’t issue or manage certs or a CA.
    

So, for **WireGuard encryption**, there is **no CA to replace** – it’s all key-pair based, auto-managed by AKS.

### b) L7 policies and ACNS security agent

For **L7 policies**, ACNS uses:

*   **Cilium eBPF** to mark and redirect traffic.
    
*   A **node-local Envoy proxy** (running in a security-agent DaemonSet) as the L7 enforcement point. [Microsoft Learn+1](https://learn.microsoft.com/en-us/azure/aks/container-network-security-l7-policy-concepts?utm_source=chatgpt.com)
    

Key points:

*   ACNS **does not inject a new CA into your workloads** or force them to trust a Microsoft-issued CA.
    
*   For **plain HTTP / gRPC / Kafka**, Envoy can inspect the traffic directly.
    
*   For **TLS’d traffic**, you typically:
    
    *   Either rely on metadata (SNI, ports, IPs) with L3/L4/FQDN policies; or
        
    *   Explicitly introduce a proxy pattern that terminates TLS (e.g. your own Envoy/ingress/service mesh) using _your_ certificates.
        

Microsoft’s docs don’t describe ACNS as a generic TLS MITM for arbitrary app traffic; it’s providing L7 enforcement on top of Cilium, not replacing your app PKI.

So from an app perspective, **ACNS doesn’t bring “a CA” you have to distribute or trust in your pods**.

## 3\. Can that CA be replaced by Vault via cert-manager or similar?

### For WireGuard encryption

*   There is **no x.509 CA** – only ephemeral keypairs managed by Azure.
    
*   The docs explicitly say the keys are _managed entirely by Azure, not by the customer_. [Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/container-network-security-wireguard-encryption-concepts)
    
*   There is currently **no public interface to plug in your own PKI / Vault keys** into WireGuard for AKS.
    

So: **No**, you cannot swap WireGuard’s key management for Vault/cert-manager today.

### For L7 / ACNS internal components

*   Internal TLS (if used between security agent components) is handled by Microsoft.
    
*   There’s no exposed setting like “ACNS CA = <this PEM bundle>” – again, no documented BYO-CA for the ACNS control plane.
    

### For _your workloads’_ TLS

This is still fully under your control:

*   You can continue to use **Vault + cert-manager** (or any PKI) for:
    
    *   In-cluster mTLS between services.
        
    *   Ingress certificates.
        
    *   External TLS termination.
        
*   You can also use **AKS custom CA trust** (\--custom-ca-trust-certificates / related settings) to make the nodes trust your internal CA for outbound calls, which is independent of ACNS. [Microsoft Learn](https://learn.microsoft.com/en-us/cli/azure/release-notes-azure-cli?view=azure-cli-latest&utm_source=chatgpt.com)
    

So the answer is:

*   **You can’t replace ACNS/WireGuard’s own crypto with Vault.**
    
*   **You** _**can**_ **continue to run your own PKI stack in parallel for application-level TLS**, and that’s the recommended pattern.
    

* * *

## 4\. What is the performance overhead?

You need to consider three layers separately: observability, L7/FQDN security, and WireGuard encryption.

### a) Container Network Observability

*   Uses eBPF and Hubble to collect flow metrics and logs. [Microsoft Azure+1](https://azure.microsoft.com/en-us/blog/enhance-the-security-and-operational-capabilities-of-your-azure-kubernetes-service-with-advanced-container-networking-services-now-generally-available/)
    
*   Overhead sources:
    
    *   eBPF programs attached to relevant hooks.
        
    *   Extra DaemonSets (Hubble, log shippers).
        
    *   Log volume into Log Analytics / Prometheus.
        
*   In practice: overhead is usually **modest but not zero**, and scales with:
    
    *   Number of flows.
        
    *   Log sampling/retention.
        
    *   Volume of L7/DNS detail you choose to collect.
        

### b) L7 policies & FQDN filtering

From the docs:

*   **Envoy is a full L7 proxy** sitting in the datapath for pods where L7 policy is enforced. [Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/container-network-security-l7-policy-concepts?utm_source=chatgpt.com)
    
*   **FQDN filtering** uses a DNS proxy in the ACNS security agent. [Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/container-network-security-fqdn-filtering-concepts?utm_source=chatgpt.com)
    

Implications:

*   **CPU / memory overhead per node**:
    
    *   Envoy is not free. Expect extra CPU and memory usage on nodes where L7 policy is heavily used.
        
    *   DNS proxy adds minimal but measurable overhead to DNS-heavy workloads, especially if you apply FQDN policies widely.
        
*   **Latency**:
    
    *   Each L7-enforced connection passes through Envoy; this introduces small extra latency and processing time.
        
    *   Enforcement complexity (e.g., complex HTTP rules) also affects cost.
        

Microsoft doesn’t publish hard SLO numbers for this, but the rule of thumb is:

> More pods behind L7 policies + more traffic volume = more overhead on Envoy and the node.

### c) WireGuard encryption (inter-node traffic only)

This is the one place where Microsoft _does_ give explicit numbers: [Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/container-network-security-wireguard-encryption-concepts)

*   It encrypts **inter-node pod traffic** only; same-node pod traffic is unencrypted.
    
*   It runs fully in software (no NIC offload).
    
*   The doc states:
    
    *   With MTU 1500, **throughput is limited to ~1.5 Gbps** per flow direction.
        
    *   With larger MTUs (e.g. 3900), they saw **~2.5× higher throughput**.
        
    *   Combining WireGuard encryption with network policies can **further reduce throughput and increase latency**.
        
*   It explicitly recommends:
    
    *   Benchmarking in non-production first.
        
    *   Paying attention to VM SKU, MTU, and traffic patterns.
        

So, overhead is definitely non-trivial for **high-bandwidth East-West traffic**; for typical microservice traffic it may be acceptable, but you must test.

* * *

## 5\. Is it completely transparent to the workload?

In terms of **application code and standard Kubernetes primitives**, yes, ACNS is designed to be transparent:

*   Pods still use:
    
    *   The same **ClusterIP/Service** endpoints.
        
    *   The same **DNS names**.
        
    *   The same application protocols.
        
*   There are **no sidecars injected** into your pods; all components are node-level DaemonSets or kernel eBPF programs.
    

The only externally visible changes for workloads are:

1.  **If you apply policies**:
    
    *   L3/L4, FQDN or L7 policies may **block or allow** traffic that was previously unrestricted.
        
    *   Pods may see **connection resets / timeouts** if a policy denies traffic.
        
2.  **If WireGuard is enabled**:
    
    *   Pod traffic to other nodes is encrypted **without any code change**.
        
    *   Applications simply see whatever effective bandwidth and latency the encrypted network provides.
        

So, from a developer’s perspective, ACNS is **functionally transparent**; from an SRE/platform perspective, it requires **new operational care** (monitoring, tuning, capacity).

* * *

## 6\. “Modify the code in a branch to enable L7 encryption”

I’ll treat this as “enable ACNS with L7 network policies (and optionally WireGuard) via IaC”, since “L7 encryption” is not a term the docs use.

### 6.1. Pre-requisites for using ACNS L7/FQDN features

From the ACNS overview & FQDN docs: [Microsoft Learn+1](https://learn.microsoft.com/en-us/azure/aks/advanced-container-networking-services-overview?utm_source=chatgpt.com)

*   **Azure CNI powered by Cilium**
    
    *   network\_plugin = "azure"
        
    *   network\_plugin\_mode = "overlay"
        
    *   network\_dataplane = "cilium"
        
*   **Kubernetes version ≥ 1.29** for security/observability features.
    
*   ACNS enabled:
    
    *   CLI: \--enable-acns
        
    *   Advanced policies: \--acns-advanced-networkpolicies L7 (or FQDN).
        


    

### 6.3. WireGuard encryption from IaC

WireGuard is:

*   Part of ACNS and based on Cilium encryption. [Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/container-network-security-wireguard-encryption-concepts)
    
*   Currently surfaced in docs via **AKS CLI and ARM**; Terraform doesn’t yet document a first-class flag in advanced\_networking.
    

Given that:

*   For now, I would treat WireGuard as a **CLI/ARM concern** driven alongside Terraform, for example:
    
    *   Create the cluster with Terraform (with ACNS & Cilium set up).
        
    *   Use a null\_resource with local-exec to run az aks update ... with the WireGuard flags, or
        
    *   Use the AzAPI provider against the relevant preview API once the WireGuard fields are exposed (likely under some encryptionProfile or similar).
        
*   I would **not** hardcode guessed property names for encryption into Terraform – follow the “Apply WireGuard encryption” doc and map that to either CLI or an AzAPI resource in your branch.
    

* * *

## 7\. How I’d structure your spike

Given your list, a pragmatic spike plan would be:

1.  **Baseline**: cluster with Azure CNI + Cilium, _without_ ACNS.
    
2.  **Enable ACNS Observability only**:
    
    *   Turn on advanced\_networking.enabled = true, observability\_enabled = true, security\_enabled = false.
        
    *   Run baseline throughput/latency tests and CPU profiling.
        
3.  **Enable ACNS Security with FQDN**:
    
    *   security\_enabled = true, advancedNetworkPolicies = "FQDN".
        
    *   Add a simple Cilium FQDN policy, validate DNS behaviour and overhead.
        
4.  **Switch to L7 policies**:
    
    *   advancedNetworkPolicies = "L7".
        
    *   Apply HTTP/gRPC L7 policies; observe:
        
        *   Failure modes when Envoy/security agent is killed.
            
        *   Additional latency for L7-protected flows.
            
5.  **Enable WireGuard**:
    
    *   Use the official instructions to turn on WireGuard in your test cluster.
        
    *   Measure:
        
        *   Max throughput between nodes (iperf3).
            
        *   Latency vs baseline.
            
        *   CPU usage on nodes under load.
            

That will give you hard data for resilience, performance, and transparency, which you can then write up against each of the questions you listed.

If you want, next step I can help you draft:

*   Example Cilium L7 and FQDN policies for ACNS.
    
*   A minimal test plan (commands + expected outputs) for your spike write-up.